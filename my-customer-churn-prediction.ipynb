{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Customer Churn Prediction using Artificial Neural Networks\n\nThis notebook demonstrates how to predict customer churn using Artificial Neural Networks (ANN). We'll use a dataset containing various customer attributes to build a predictive model that can identify customers who are likely to leave a service.","metadata":{}},{"cell_type":"code","source":"# Import necessary libraries\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O\nimport matplotlib.pyplot as plt # for visualization\nimport seaborn as sns # for statistical data visualization\n\n# For model building and evaluation\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report, roc_curve, auc\n\n# For deep learning\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom keras import Sequential\nfrom keras.layers import Dense, Dropout\nfrom keras.callbacks import EarlyStopping\n\n# For handling imbalanced data\nfrom imblearn.over_sampling import SMOTE\n\n# Set random seed for reproducibility\nnp.random.seed(42)\ntf.random.set_seed(42)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Data Loading and Exploration\n\nFor this project, we'll use a synthetic customer churn dataset that mimics real-world banking customer data.","metadata":{}},{"cell_type":"code","source":"# Create a synthetic customer churn dataset\n# In a real scenario, you would load your data from a file\n# For example: df = pd.read_csv('/path/to/churn_data.csv')\n\n# Generate synthetic data\nn_samples = 10000\n\n# Customer ID\ncustomer_id = np.arange(1, n_samples + 1)\n\n# Demographics\nage = np.random.normal(40, 10, n_samples).round().astype(int)\nage = np.clip(age, 18, 95)  # Clip to reasonable age range\n\ngender = np.random.choice(['Male', 'Female'], n_samples)\n\n# Geographic information\ncountry = np.random.choice(['France', 'Spain', 'Germany'], n_samples, p=[0.5, 0.3, 0.2])\n\n# Account information\ncredit_score = np.random.normal(650, 100, n_samples).round().astype(int)\ncredit_score = np.clip(credit_score, 300, 850)  # Clip to reasonable credit score range\n\ntenure = np.random.poisson(5, n_samples)  # Years with the bank\nbalance = np.random.exponential(50000, n_samples).round(2)  # Account balance\nnum_products = np.random.choice([1, 2, 3, 4], n_samples, p=[0.5, 0.3, 0.15, 0.05])  # Number of bank products\nhas_credit_card = np.random.choice([0, 1], n_samples, p=[0.3, 0.7])  # Has a credit card\nis_active_member = np.random.choice([0, 1], n_samples, p=[0.2, 0.8])  # Active member\nestimated_salary = np.random.normal(70000, 30000, n_samples).round(2)  # Estimated salary\n\n# Create features that influence churn\nchurn_prob = 0.2 - 0.01 * tenure + 0.1 * (num_products > 2).astype(int) - 0.05 * is_active_member + 0.1 * (balance < 10000).astype(int)\nchurn_prob = np.clip(churn_prob, 0.05, 0.95)  # Ensure probabilities are between 0.05 and 0.95\n\n# Generate churn based on calculated probabilities\nchurn = np.random.binomial(1, churn_prob)\n\n# Create DataFrame\ndata = {\n    'CustomerId': customer_id,\n    'CreditScore': credit_score,\n    'Gender': gender,\n    'Age': age,\n    'Tenure': tenure,\n    'Balance': balance,\n    'NumOfProducts': num_products,\n    'HasCrCard': has_credit_card,\n    'IsActiveMember': is_active_member,\n    'EstimatedSalary': estimated_salary,\n    'Geography': country,\n    'Exited': churn\n}\n\ndf = pd.DataFrame(data)\n\n# Display the first few rows\ndf.head()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Check the shape of the dataset\ndf.shape","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Get information about the dataset\ndf.info()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Statistical summary of the dataset\ndf.describe()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Check for missing values\ndf.isnull().sum()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Check for duplicates\ndf.duplicated().sum()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Check the distribution of the target variable (churn)\nplt.figure(figsize=(8, 6))\nsns.countplot(x='Exited', data=df)\nplt.title('Distribution of Customer Churn')\nplt.xlabel('Exited (0 = No, 1 = Yes)')\nplt.ylabel('Count')\nplt.show()\n\n# Print the percentage of each class\nchurn_percentage = df['Exited'].value_counts(normalize=True) * 100\nprint(f\"Percentage of customers who stayed: {churn_percentage[0]:.2f}%\")\nprint(f\"Percentage of customers who churned: {churn_percentage[1]:.2f}%\")","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Exploratory Data Analysis","metadata":{}},{"cell_type":"code","source":"# Correlation heatmap for numerical features\nnumerical_features = df.select_dtypes(include=['int64', 'float64']).drop('CustomerId', axis=1)\n\nplt.figure(figsize=(12, 10))\nsns.heatmap(numerical_features.corr(), annot=True, cmap='coolwarm', linewidths=0.5)\nplt.title('Correlation Heatmap of Numerical Features')\nplt.show()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Age distribution by churn status\nplt.figure(figsize=(10, 6))\nsns.histplot(data=df, x='Age', hue='Exited', multiple='stack', bins=20)\nplt.title('Age Distribution by Churn Status')\nplt.xlabel('Age')\nplt.ylabel('Count')\nplt.show()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Balance distribution by churn status\nplt.figure(figsize=(10, 6))\nsns.histplot(data=df, x='Balance', hue='Exited', multiple='stack', bins=20)\nplt.title('Balance Distribution by Churn Status')\nplt.xlabel('Balance')\nplt.ylabel('Count')\nplt.show()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Churn rate by geography\nplt.figure(figsize=(10, 6))\nsns.barplot(x='Geography', y='Exited', data=df, estimator=np.mean)\nplt.title('Churn Rate by Geography')\nplt.xlabel('Country')\nplt.ylabel('Churn Rate')\nplt.show()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Churn rate by number of products\nplt.figure(figsize=(10, 6))\nsns.barplot(x='NumOfProducts', y='Exited', data=df, estimator=np.mean)\nplt.title('Churn Rate by Number of Products')\nplt.xlabel('Number of Products')\nplt.ylabel('Churn Rate')\nplt.show()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Churn rate by active membership status\nplt.figure(figsize=(10, 6))\nsns.barplot(x='IsActiveMember', y='Exited', data=df, estimator=np.mean)\nplt.title('Churn Rate by Active Membership Status')\nplt.xlabel('Is Active Member (0 = No, 1 = Yes)')\nplt.ylabel('Churn Rate')\nplt.show()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Churn rate by gender\nplt.figure(figsize=(10, 6))\nsns.barplot(x='Gender', y='Exited', data=df, estimator=np.mean)\nplt.title('Churn Rate by Gender')\nplt.xlabel('Gender')\nplt.ylabel('Churn Rate')\nplt.show()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Tenure vs Churn\nplt.figure(figsize=(10, 6))\nsns.boxplot(x='Exited', y='Tenure', data=df)\nplt.title('Tenure by Churn Status')\nplt.xlabel('Exited (0 = No, 1 = Yes)')\nplt.ylabel('Tenure (Years)')\nplt.show()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Data Preprocessing","metadata":{}},{"cell_type":"code","source":"# Drop customer ID as it's not relevant for prediction\ndf_model = df.drop('CustomerId', axis=1)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Separate features and target variable\nX = df_model.drop('Exited', axis=1)\ny = df_model['Exited']","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Identify categorical and numerical columns\ncategorical_cols = X.select_dtypes(include=['object']).columns.tolist()\nnumerical_cols = X.select_dtypes(include=['int64', 'float64']).columns.tolist()\n\nprint(f\"Categorical columns: {categorical_cols}\")\nprint(f\"Numerical columns: {numerical_cols}\")","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Create preprocessing pipelines for both numerical and categorical data\nnumerical_transformer = StandardScaler()\ncategorical_transformer = OneHotEncoder(drop='first')\n\n# Combine preprocessing steps\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', numerical_transformer, numerical_cols),\n        ('cat', categorical_transformer, categorical_cols)\n    ])\n\n# Apply preprocessing\nX_processed = preprocessor.fit_transform(X)\n\n# Get feature names after one-hot encoding\ncat_feature_names = preprocessor.named_transformers_['cat'].get_feature_names_out(categorical_cols)\nfeature_names = numerical_cols + list(cat_feature_names)\nprint(f\"Features after preprocessing: {feature_names}\")","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X_processed, y, test_size=0.2, random_state=42, stratify=y)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Check for class imbalance\nprint(f\"Training set class distribution:\\n{pd.Series(y_train).value_counts(normalize=True)}\")\nprint(f\"\\nTesting set class distribution:\\n{pd.Series(y_test).value_counts(normalize=True)}\")","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Apply SMOTE to handle class imbalance\nsmote = SMOTE(random_state=42)\nX_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n\n# Check the new class distribution\nprint(f\"After SMOTE, training set class distribution:\\n{pd.Series(y_train_resampled).value_counts(normalize=True)}\")","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Building the Neural Network Model","metadata":{}},{"cell_type":"code","source":"# Define the model architecture\nmodel = Sequential()\n\n# Input layer and first hidden layer\nmodel.add(Dense(16, activation='relu', input_dim=X_train_resampled.shape[1]))\nmodel.add(Dropout(0.3))  # Add dropout for regularization\n\n# Second hidden layer\nmodel.add(Dense(8, activation='relu'))\nmodel.add(Dropout(0.2))\n\n# Output layer with sigmoid activation for binary classification\nmodel.add(Dense(1, activation='sigmoid'))","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Model summary\nmodel.summary()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Compile the model\nmodel.compile(\n    optimizer='adam',\n    loss='binary_crossentropy',\n    metrics=['accuracy']\n)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Define early stopping callback\nearly_stopping = EarlyStopping(\n    monitor='val_loss',\n    patience=10,\n    restore_best_weights=True\n)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Train the model\nhistory = model.fit(\n    X_train_resampled, y_train_resampled,\n    epochs=50,\n    batch_size=32,\n    validation_split=0.2,\n    callbacks=[early_stopping],\n    verbose=1\n)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Model Evaluation","metadata":{}},{"cell_type":"code","source":"# Plot training history\nplt.figure(figsize=(12, 5))\n\n# Plot training & validation loss values\nplt.subplot(1, 2, 1)\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('Model Loss')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Validation'], loc='upper right')\n\n# Plot training & validation accuracy values\nplt.subplot(1, 2, 2)\nplt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.title('Model Accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Validation'], loc='lower right')\n\nplt.tight_layout()\nplt.show()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Make predictions on the test set\ny_pred_proba = model.predict(X_test)\ny_pred = (y_pred_proba > 0.5).astype(int)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Calculate evaluation metrics\naccuracy = accuracy_score(y_test, y_pred)\nprecision = precision_score(y_test, y_pred)\nrecall = recall_score(y_test, y_pred)\nf1 = f1_score(y_test, y_pred)\n\nprint(f'Accuracy: {accuracy:.4f}')\nprint(f'Precision: {precision:.4f}')\nprint(f'Recall: {recall:.4f}')\nprint(f'F1 Score: {f1:.4f}')","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Generate confusion matrix\ncm = confusion_matrix(y_test, y_pred)\n\n# Plot confusion matrix\nplt.figure(figsize=(8, 6))\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False)\nplt.title('Confusion Matrix')\nplt.ylabel('Actual')\nplt.xlabel('Predicted')\nplt.show()\n\n# Calculate and display metrics from confusion matrix\ntn, fp, fn, tp = cm.ravel()\ntotal = tn + fp + fn + tp\n\nprint(f\"True Negatives: {tn} ({tn/total:.2%})\")\nprint(f\"False Positives: {fp} ({fp/total:.2%})\")\nprint(f\"False Negatives: {fn} ({fn/total:.2%})\")\nprint(f\"True Positives: {tp} ({tp/total:.2%})\")","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Print classification report\nprint(classification_report(y_test, y_pred))","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Plot ROC curve\nfpr, tpr, thresholds = roc_curve(y_test, y_pred_proba)\nroc_auc = auc(fpr, tpr)\n\nplt.figure(figsize=(10, 8))\nplt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')\nplt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver Operating Characteristic (ROC) Curve')\nplt.legend(loc='lower right')\nplt.show()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Feature Importance Analysis","metadata":{}},{"cell_type":"code","source":"# Create a simple function to estimate feature importance using permutation importance\ndef get_feature_importance(model, X, y, feature_names):\n    # Initialize importance array\n    importances = []\n    baseline_accuracy = accuracy_score(y, (model.predict(X) > 0.5).astype(int))\n    \n    # For each feature\n    for i in range(X.shape[1]):\n        # Create a copy of the data\n        X_permuted = X.copy()\n        \n        # Shuffle the values of the current feature\n        np.random.shuffle(X_permuted[:, i])\n        \n        # Predict with the permuted feature\n        y_pred_permuted = (model.predict(X_permuted) > 0.5).astype(int)\n        \n        # Calculate the decrease in accuracy\n        permuted_accuracy = accuracy_score(y, y_pred_permuted)\n        importance = baseline_accuracy - permuted_accuracy\n        importances.append(importance)\n    \n    # Create a DataFrame with feature names and importance scores\n    feature_importance = pd.DataFrame({\n        'Feature': feature_names,\n        'Importance': importances\n    })\n    \n    # Sort by importance\n    feature_importance = feature_importance.sort_values('Importance', ascending=False)\n    \n    return feature_importance\n\n# Get feature importance\nfeature_importance = get_feature_importance(model, X_test, y_test, feature_names)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Plot feature importance\nplt.figure(figsize=(12, 8))\nsns.barplot(x='Importance', y='Feature', data=feature_importance.head(10))\nplt.title('Top 10 Feature Importance')\nplt.tight_layout()\nplt.show()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Customer Churn Prediction Example","metadata":{}},{"cell_type":"code","source":"# Create a function to predict churn for a new customer\ndef predict_churn(customer_data):\n    # Preprocess the customer data\n    customer_processed = preprocessor.transform(pd.DataFrame([customer_data]))\n    \n    # Make prediction\n    churn_probability = model.predict(customer_processed)[0][0]\n    churn_prediction = 1 if churn_probability > 0.5 else 0\n    \n    return churn_prediction, churn_probability\n\n# Example customer\nnew_customer = {\n    'CreditScore': 650,\n    'Gender': 'Female',\n    'Age': 35,\n    'Tenure': 2,\n    'Balance': 25000,\n    'NumOfProducts': 3,\n    'HasCrCard': 1,\n    'IsActiveMember': 0,\n    'EstimatedSalary': 65000,\n    'Geography': 'France'\n}\n\n# Predict churn for the new customer\nchurn_prediction, churn_probability = predict_churn(new_customer)\n\nprint(f\"Churn Prediction: {'Yes' if churn_prediction == 1 else 'No'}\")\nprint(f\"Churn Probability: {churn_probability:.2%}\")","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Conclusion\n\nIn this notebook, we built an Artificial Neural Network model to predict customer churn based on various customer attributes. The model achieved an accuracy of [value] and an F1 score of [value], indicating its effectiveness in identifying customers who are likely to churn.\n\nThe most important factors affecting customer churn were found to be:\n1. [Top factor based on feature importance]\n2. [Second factor based on feature importance]\n3. [Third factor based on feature importance]\n\nThis model could be used by businesses to identify customers at risk of churning and take proactive measures to retain them.","metadata":{}},{"cell_type":"markdown","source":"## Future Work\n\n1. Try different model architectures and hyperparameters\n2. Implement more sophisticated feature engineering\n3. Explore other techniques for handling class imbalance\n4. Develop a customer retention strategy based on the model's predictions\n5. Deploy the model as a real-time prediction service","metadata":{}}]}